# Loading and running models from C

We want a light-weight library to load into larger applications that only need to execute the tensorflow models. Building a C++ application that links to the standard tensorflow install requires adding it to the bazel build system, which is intractable for massive simulation codes.

The static library produced by 
[https://github.com/FloopCZ/tensorflow_cc](https://github.com/FloopCZ/tensorflow_cc)
results in a 100MB executable to just say helloworld! That's too much.
The static library is only for C++ as well, which might be a problem trying to link to a Fortran program.
(Well, to be fair, the problem is Fortran.)
The compilation also didn't work on macOS, yielding unknown instruction errors for the vector operations.
The TensorFlow build scripts don't have a dedicated mac version.
It's not a big deal because I like to run in Docker on mac anyways, but there is another solution.


TensorFlow provides a slimmed down  precompiled library with an API for C:
[https://www.tensorflow.org/install/install_c](https://www.tensorflow.org/install/install_c)
libtensorflow.so is only 49MB and lets us go with the dynamic linking option. There are precompiled options for macOS, too, but I haven't tried them yet.

## Loading a graph

The meta graphs don't work in the C API because apparently they have some Python elements.
Instead, we have to serialize the graph into the protobuf description.
I had to add a line to the end of the model_loading notebook to write it out differently,
```python
gd=graph5.as_graph_def()
from tensorflow.python.platform import gfile
with gfile.GFile("trimmed.pb", "w") as f:
	f.write(gd.SerializeToString())
```
The protobuf files can be loaded by the C API. The file [load_graph.c](load_graph.c) is the first working
example I found from
[stackoverflow](https://stackoverflow.com/questions/41688217/how-to-load-a-graph-with-tensorflow-so-and-c-api-h-in-c-language), credited to user `ash`.


## Executing a graph

After some more digging, I found some more [example code on StackOverflow](https://stackoverflow.com/questions/44305647/segmentation-fault-when-using-tf-sessionrun-to-run-tensorflow-graph-in-c-not-c) that builds on that last bit of code, credited to user `DrBBQ`. (StackOverflow is truly the modern way of learning libraries.) I believe this code has a bug in it due to its use of a stack-allocated array for `float values[]` and then passing in a `Deallocator` that calls free. 


## A first-draft library

I cleaned up the code and introduced a simple object, `PopModel` in [clean.c](clean.c).
The idea is to have a simple struct that encapsulates reading a graph from the
protobuf file and manages all of the state data:
```C
typedef struct pop_model_str {
  TF_Graph* graph;
  TF_Output inputs[INP_MAX];
  TF_Tensor * in_tens;
  size_t in_length;
  TF_Output outputs[OUT_MAX];
  TF_Tensor * out_tens;
  size_t out_length;
  TF_Status * status;
  TF_SessionOptions * opts;
  TF_Session* sess;
} pop_model_t;
int PopModel_Init(pop_model_t * self, char * fname)
void PopModel_Destroy(pop_model_t * self);
void PopModel_Eval(pop_model_t * self, real_t * input, real_t * output);
```
All of the TF objects needed are stored inside of the struct in a way that
no memory allocations should have to happen inside of `PopModel_Eval`, though I can't speak for what happens inside of `TF_SessionRun`.

The test code inside of `main()` that loads the graph we made in the last notebook is
```C
pop_model_t myModel;
int status;
status = PopModel_Init(&myModel, "../trimmed.pb");
if(status) return status;
real_t input[2] = { 3.0, 1.2 };
real_t output[1];
PopModel_Eval(&myModel, input,output);
printf("f(%f,%f) = %f\n",input[0],input[1], output[0]);
PopModel_Destroy(&myModel);
```

To figure out the dimensions of a calculation, TensorFlow needs to traverse the graph. The code to determine the size of an output node and allocate a Tensor that can store the data is:
```C
int nodims = TF_GraphGetTensorNumDims(self->graph,self->outputs[0],self->status);
int64_t odims[nodims];
TF_GraphGetTensorShape(self->graph,self->outputs[0],odims,nodims,self->status);
if(odims[0]==-1) odims[0]=1;
for(int i=0;i<nodims;i++) num_bytes_out*=odims[i];
num_bytes_out *= sizeof(float);
self->out_tens= TF_AllocateTensor(TF_DOUBLE, odims, nodims, num_bytes_out);
```


## Statefullness of Our Computations

We eventually want to put this ability into computational kernels, like those generated by
[Popcorn](https://github.com/afqueiruga/popcorn).
However, Popcorn has a strict no-side-effect, stateless policy to the kernels.
Strictly adhering to that policy would demand that the kernels constantly open and close tensorflow sessions
and read from the file.
That's obviously a horrible inefficiency and not at all how tensorflow is designed to be used.
The kernels will have to deviate from this pureness and add a scratch buffer and have state information.
I was already thinking about this for kernels that call the Gnu Scientific Library, which requires some preallocated data structs
for scratch space, e.g. the perumation vector for LU decomposition.
Every spawned thread from cornflakes will have to allocate an independent scratch buffer, and kernels will
have to define some sort of autogenerated struct that maps onto the buffer.
The above `PopModel` was designed with this in mind, where the struct would go inside of this buffer and the `Init` and `Destroy` methods would be called by the scratch space handling of the kernel.
I hate the idea of changing the kernel eval routine to have a `*ctx` pointer though! Ughh.

## Where do we go from here?

Now, I am going to clean up this library even more. What we need is:

1. Freeze the model
2. Compute the gradients of the inputs / outputs
3. Make the protobuf file
4. Have a simple library like `clean.c` that reads it and calls Eval and GradEval methods

I still fear that the overhead of `TF_SessionRun` is too much for the ultimate goal of jamming these models into a scientific code.
Some profiling and speed comparisons need to be done to evaluate the overhead of the session abstraction when I have a working simulation.
Ideally, I would like to be able to unfold a frozen graph into a method that looks like,

```C
const float W1[] = {1.0,1.0...}, W2[] = {...};
const float b1[] = {2.0,2.0...}, b2[] = {...};
void model_eval(float * inp, float * out) {
  float h1[], h2[] ...;
  matmul(W1,inp,h1);
  axpy(h1,1,b1);
  relu(h1);
  matmul(W2,h2,out);
}
```

I.e., transforming the TensorFlow graph into a C routine with the frozen parameters included in the object file and intermediate values on the graph allocated on the stack. But, premature optimization is the root of all evil.
